# 大模型是发展通用人工智能的重要途径
## 专用模型的发展路程
专用模型是针对特定任务，一个模型解决一个特定问题
1. 2006 深度学习理论突破（深度置信网络）
2. 2011 大规模语音识别（Switchboard 错误率降低）
3. 2012 ImageNet竞赛（1000类，100万数据）
4. 2014 人脸识别（LFW识别率99%，超过人类）
5. 2016 围棋比赛（AlphaGo 4：1李世石）
6. 2019 德州扑克（首次在多人复杂对局中超越人类）
7. 2021 AlphaFold （蛋白质结构预测准确率新高）
## 通用大模型
一个模型应对多种任务、多种模态
1. chatgpt
2. gpt 4o

# 书生浦语大模型开源历程

1. 23.6.7 InternLM 千亿参数语言大模型发布
2. 23.7.6 InternLM 千亿参数大模型升级 支持8K语境、26种语言（全面开源，免费商用）InternLM-7B模型、全链条开源工具体系
3. 23.8.14 书生万卷1.0 多模态预训练语料库开源发布
4. 23.8.21 升级版对话模型 InternLM-Chat-7B v1.1 发布 开源智能体框架Lagent 支持从语言模型到智能体升级转换
5. 23.8.28 InternLM 千亿参数大模型 参数量升级到123B
6. 23.9.20 增强版InternLM-20B 开源 开源工具链全线升级
7. 24.1.17 InternLM 2 开源

# 书生浦语2.0 （InternLM2）的体系

面对不同的使用需求，每个规格包含三个模型版本
7B：为轻量级的研究和应用提供了一个轻便但性能不俗的模型
20B：模型的综合性能更加强劲，可有效支持更加复杂的使用场景
InternLM2-Base ：高质量和具有很强可塑性的模型基座，是模型进行深度领域适配的高质量起点
InternLM：在Base基础上，在多个能力方向进行了强化，在评测中成绩优异，同时保持了很好的通用语言能力，是我们推荐的大部分应用中考虑选用的优秀基座
InternLM-Chat：在Base基础上，经过SFT和RLHF，面对对话交互进行了优化，具有很好的指令遵循、共情聊天和调用工具等的能力

# InternLM2的
##新一代数据清洗过滤的技术
1 多维度数据价值评估
基于文本质量、信息质量、信息密度等维度对数据价值进行综合评估和提升
2 高质量语料驱动的数据富集
利用高质量语料的特征从物理世界、互联网以及语料库中进一步富集更多类似语料
3 有针对性的数据补齐
针对性补充语料，重点加强世界知识、数理、代码等核心能力

Loss 分布显示二代比一代左移，表明二代比一代有更好的性能。

# InternLM2的主要亮点
1. 超长上下文（模型在20万token上下文）
2. 综合性能全面提升（推理、数学 、代码提升显著InternLM2-Chat-20B在重点评测走上比肩ChatGPT）
3. 优秀的对话和创作体验（精准指令跟随，丰富的结构化创作，在AlphaEval2超越GPT3.5和Gemini Pro）
4. 工具调用能力整体升级（可靠支持工具多轮调用，复杂智能体搭建）
5. 突出的数理能力和实用的数据分析功能（强大的内生计算能力，加入代码解释后，在GSMK8K和MATH达到和GPT-4相仿水平）

# 性能全方位提升

在各个能力维度全面提升，在推理、数学、代码等方面的能力提升尤为显著，综合性能达到同量级开源模型的领先水平，在重点能力评测上InternLM2-Chat-20B甚至可以达到比肩ChatGPT的水平![](https://github.com/SunXiaoXiang/interAI_2ndbat/blob/main/img/1/1.png)

从性能对比上看，无论是轻量级还是中量级，考试、语言和推理都很高评分，几乎满足日常使用，但是数学、代码等还是一般，仅30分左右。

# 几个日常使用的场景
1. AI助手（作为旅游助手，帮助安排日程，可规划路径并展示景点和费用，整个计划可行性高且贴近日常实际）
2. 心理咨询顾问（日常抚慰，给予情绪价值，充满人文关怀的对话，让chat有温度）
3. 剧本创作（在限定题材和范围内，提供剧本创作能力）

# 工具调用能力提升

工具调用拓展了大数据模型的能力边界，使得大语言模型能够通过搜索、计算、代码解释器等获得最新的知识并处理更加复杂的问题。

# 强大的内生计算能力
1. 高准确率（在不依靠计算器等外部工具下，在100以内的简单数学计算接近100%的准确率，在1000以内达到80%左右的计算准确率   -  感觉不太行，实际中使用并不可靠的）
2. 复杂运算和求解（能够进行复杂数学的运算和求解 - 测试题目是高等数学和微积分等）

# 代码解释器：更上一层楼
在典型的数学评测集GSM8K和MATH上，配合代码解释器，InternLM2可以进一步提升得分。其中对于难度更高的MATH数据集，借助代码解释器，精度从32.5提升到51.2，超过GPT-4的表现。
![](https://github.com/SunXiaoXiang/interAI_2ndbat/blob/main/img/1/2.png)

# 实用的数据分析功能

可以上传表格，通过InterLM2来完成数据的分析，并完成数据预测或趋势分析。

# 从模型到应用
![](https://github.com/SunXiaoXiang/interAI_2ndbat/blob/main/img/1/3.png)

# 书生浦语全链条开源开放体系
1. 数据（书生 万卷， 2TB数据，涵盖多种模态和任务）
2. 预训练（InternLM-Train 并行训练，极致优化 速度可以达到3600 tokens/sec/gpu）
3. 微调XTurner（支持全参数微调，支持LoRA等低成本微调）
4. 部署LMDeploy（全链路部署，性能领先，每秒生成2000+ tokens）
5. 评测OpenCompass（全方位评测，性能可复现，100套评测集，50万道题目）
6. 应用Lagent AgentLego（支持多种智能体，支持代码解释器等多种工具）
7. 

## 《InternLM2技术报告》
深入探讨了InternLM2开源大型语言模型，该模型通过创新的预训练策略、数据筛选技术和优化方法，在多个维度和广泛应用场景中实现了显著的性能提升。

### 模型背景与目标

随着ChatGPT和GPT-4等大型语言模型(LLMs)的出现，有关人工通用智能(AGI)到来的讨论日益增多。然而，开源社区在复制这些尖端成果时面临挑战。在此背景下，InternLM2作为一款开源LLM，旨在通过全面的预训练和优化技术，提供一个在多项指标上超越现有模型的解决方案。该模型不仅在标准基准测试中表现优异，还在长上下文理解和开放性主观评估上展示出非凡能力。

### 预训练过程与技术创新

#### 多元数据整合

InternLM2的预训练数据涵盖了文本、代码以及特定的长上下文数据，这要求模型能够理解不同领域的知识并处理复杂的语言结构。特别是，它利用书籍和GitHub代码仓库中的长文本数据，通过深度优先搜索排序和文件内容简述生成，精心构造训练样本，确保模型在长文本理解方面的能力得到加强。

#### 数据筛选机制

为了确保数据质量，InternLM2团队采用了一系列创新的数据筛选方法：
- **大小过滤**：移除超过32K字节的数据样本，以避免过大的内存负担。
- **统计过滤**：基于词汇和语言学特征，排除不符合规则的样本，例如通过检查连词使用频率来识别不连贯的文本段落。
- **困惑度过滤**：通过比较文本段落间的条件概率差异（即困惑度），剔除引入干扰性上下文的样本，保证数据的连贯性和逻辑性。

### 长上下文微调与代码能力增强

为了维持模型在微调后的长上下文处理能力，InternLM2在监督式微调(SFT)和基于强化学习的反馈循环(RLHF)中继续使用长上下文预训练数据。特别是在增强代码理解与生成能力方面，模型选择了DS-1000数据集中的核心代码库，如Pandas、Numpy等，并从GitHub上挑选高星标引用这些库的项目，经过严格的数据清洗和预处理后，将文件内容按顺序拼接，形成长度达到32K字节的长上下文代码数据，显著提升了模型在代码分析和生成任务上的表现。

### 工具辅助的LLMs与奖励模型优化

InternLM2引入了修改后的ChatML格式，以“环境”角色支持通用工具调用，增强了模型在特定情境下的应用能力。同时，报告还进行了一项关于条件奖励模型的消融研究，证明了在模型训练中使用条件系统提示的重要性。对比实验显示，加入条件提示能显著提高模型在多领域数据集上的精确度，涉及有益无害对话、内容摘要、数学问题求解及Reddit回复等多个场景。

### 性能评估与讨论

通过对包括IFEval在内的多个公开数据集的评估，InternLM2在不同参数规模下展现出领先的性能。尽管英语模型如Llama2和Mistral在某些基准测试上略有优势，但InternLM2在7B和13-20B参数阶段分别排名第二和第三，表明其在指令遵循任务方面的挑战下仍保持了与同类模型相比的领先地位。

### 结论

综上所述，InternLM2通过精心设计的预训练策略、创新的数据处理流程和针对性的微调方法，在多个维度上取得了显著的性能提升。它不仅在传统语言理解任务上表现出色，还在代码理解和生成、长文本处理以及工具辅助功能上展示了强大的能力。这份报告强调了开源LLMs在追求更高级别的人工智能性能时的潜力，为推动AGI的发展提供了重要的开源贡献。
